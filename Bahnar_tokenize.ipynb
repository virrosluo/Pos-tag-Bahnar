{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \".\\\\Bahnar_dataset\"\n",
    "folder = \"KH-CN\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Bahnaric</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>PoS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>abĕn</td>\n",
       "      <td>váy</td>\n",
       "      <td>Noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>abŭt</td>\n",
       "      <td>băo</td>\n",
       "      <td>Verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ada</td>\n",
       "      <td>vịt</td>\n",
       "      <td>Noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>adal adal</td>\n",
       "      <td>cẩn thận, khe khẽ, thong thả</td>\n",
       "      <td>Adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>adon</td>\n",
       "      <td>xếp</td>\n",
       "      <td>Verb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No.   Bahnaric                    Vietnamese    PoS tag\n",
       "0    1       abĕn                           váy       Noun\n",
       "1    2       abŭt                           băo       Verb\n",
       "2    3        ada                           vịt       Noun\n",
       "3    4  adal adal  cẩn thận, khe khẽ, thong thả  Adjective\n",
       "4    5       adon                           xếp       Verb"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_path = path + \"\\\\Kriem-Common_PoS.xlsx\"\n",
    "dict_frame =pd.read_excel(dict_path)\n",
    "\n",
    "dict_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_word(dict, word, value):\n",
    "    if word in dict:\n",
    "        # Check for tag\n",
    "        not_Contain = 1\n",
    "        for i in dict[word][1]:\n",
    "            if value[1] == i:\n",
    "                not_Contain = 0\n",
    "                break\n",
    "        if not_Contain == 1:\n",
    "            dict[word][1].append(value[1])\n",
    "            dict[word][0].append(value[0])\n",
    "            print(f\"{word} with Tag: {len(dict[word][1])}\")            \n",
    "    else:\n",
    "        dict[word] = [[], []]\n",
    "        dict[word][0].append(value[0])\n",
    "        dict[word][1].append(value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba with Tag: 2\n",
      "bal with Tag: 2\n",
      "bĕng with Tag: 2\n",
      "bih with Tag: 2\n",
      "bŏ with Tag: 2\n",
      "blah with Tag: 2\n",
      "hăp with Tag: 2\n",
      "hlak with Tag: 2\n",
      "kăp with Tag: 2\n",
      "klaih with Tag: 2\n",
      "kung with Tag: 2\n",
      "luôn with Tag: 2\n",
      "mơ̆u with Tag: 2\n",
      "nhŭ with Tag: 2\n",
      "sil with Tag: 2\n",
      "sĭn with Tag: 2\n",
      "sră with Tag: 2\n",
      "tŏ with Tag: 2\n",
      "drŏng with Tag: 2\n",
      "hlak with Tag: 3\n",
      "weh with Tag: 2\n",
      "yôm with Tag: 2\n"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "for idx, str in enumerate(dict_frame['Bahnaric']):\n",
    "    lst_word = str.split(',')\n",
    "    contend = dict_frame.iloc[idx][2:4]\n",
    "    for word in lst_word:\n",
    "        word = word.strip()\n",
    "        push_word(dict, word, contend)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "file_name = []\n",
    "dataset = []\n",
    "dataset_path = path + \"\\\\\" + folder\n",
    "for obj in os.scandir(dataset_path):\n",
    "    dataframe = pd.read_excel(dataset_path + \"\\\\\" + obj.name)\n",
    "    dataset.append((obj.name, dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KH-CN bai 03.xlsx\n",
      "   ID                                         Tiếng Việt  \\\n",
      "0   1  Quyết định về việc phê duyệt Kế hoạch lựa chọn...   \n",
      "1   2  Xây dựng mô hình ứng dụng tiến bộ khoa học côn...   \n",
      "2   3   Căn cứ Luật Tổ chức HĐND và UBND ngày 26/11/2003   \n",
      "3   4  Căn cứ Luật Đấu thầu số 43/2013/QH13 ngày 26/1...   \n",
      "4   5  Căn cứ Nghị định số 63/2014/NĐ-CP ngày 26/6/20...   \n",
      "\n",
      "                                          Tiếng BaNa  \n",
      "0  Kuiê̆t đĭnh lơm dron gphê yuiêt Kê̆ hoach...  \n",
      "1  Pơjing mô hinh ư̆ng yŭng tiê̆n 'bô̆ khoa h...  \n",
      "2  Kăn kư̆ Luât tô chưk HNĐND xưm UBND năr 'b...  \n",
      "3  Kăn kư̆ Luât đâ̆u thâu sô̆ puôn jĭt pê...  \n",
      "4  Kăn kư̆ Nghĭ đĭnh sô̆ drâu jĭt pêng xơn...  \n"
     ]
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "print(a)\n",
    "print(b.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence_Info():\n",
    "    def __init__(self, bana_token, bana_tag, vietnam_token, vietnam_tag, miscount) -> None:\n",
    "        self.bana_token = bana_token\n",
    "        self.bana_tag = bana_tag\n",
    "\n",
    "        self.vietnam_token = vietnam_token\n",
    "        self.vietnam_tag = vietnam_tag\n",
    "\n",
    "        self.miscount = miscount\n",
    "\n",
    "    def get_bana(self):\n",
    "        return self.bana_token\n",
    "    \n",
    "    def get_vietnamese(self):\n",
    "        return self.vietnam_token\n",
    "    \n",
    "    def get_miscount(self):\n",
    "        return self.miscount\n",
    "    \n",
    "    def write_excel(self, fp):\n",
    "        fp.write(f\"{self.miscount}\\n\")\n",
    "        \n",
    "\n",
    "        token_str = self.bana_token[0]\n",
    "        tag_str = '/'.join(self.bana_tag[0])\n",
    "        for token, tag in zip(self.bana_token[1:], self.bana_tag[1:]):\n",
    "            tag_str = tag_str + \",\" + '/'.join(tag)\n",
    "            token_str = token_str + \",\" + token\n",
    "\n",
    "        fp.write(f\"{token_str}\\n\")\n",
    "        fp.write(f\"{tag_str}\\n\")\n",
    "\n",
    "        fp.write(f\"{','.join(self.vietnam_token)}\\n\")\n",
    "        fp.write(f\"{','.join(self.vietnam_tag)}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_tag_vietnam(sequence):\n",
    "    token, tag = ViPosTagger.postagging(ViTokenizer.tokenize(sequence))\n",
    "    return token, tag\n",
    "\n",
    "def token_tag_bahnar(sequence):\n",
    "    token_list = []\n",
    "    tag_list = []\n",
    "\n",
    "    words = word_tokenize(sequence)\n",
    "    miscount = 0\n",
    "        \n",
    "    i = 0\n",
    "    while(i < len(words)):\n",
    "        j = 1\n",
    "        while (i + j <= len(words)):\n",
    "            if ' '.join(words[i : i + j]) in dict:\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        # In case this word is in dict => write into a file and then mark the pos tag for it\n",
    "        if j > 1:\n",
    "            token = ' '.join(words[i : i + j - 1])\n",
    "\n",
    "            token_list.append(token)\n",
    "            tag_list.append(dict[token][1])\n",
    "            \n",
    "        # In case the token is not in dict => write the token and mark it\n",
    "        else:\n",
    "            miscount += 1\n",
    "            token_list.append(words[i])\n",
    "            tag_list.append(\" \")\n",
    "\n",
    "        i += j\n",
    "\n",
    "    return token_list, tag_list, miscount\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_missword(info):\n",
    "    return info.miscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_file = f\"{folder}_info\"\n",
    "if not os.path.isdir(f\"{path}\\\\{contain_file}\"):\n",
    "    os.makedirs(f\"{path}\\\\{contain_file}\")\n",
    "\n",
    "for file_name, contend in dataset:\n",
    "    sentence_lst = []\n",
    "    for idx, paragraph in contend.iterrows():\n",
    "        if pd.notna(paragraph[2]) and pd.notna(paragraph[1]):\n",
    "\n",
    "            viet_token, viet_tag = token_tag_vietnam(paragraph[1])\n",
    "\n",
    "            bana_token, bana_tag, miscount = token_tag_bahnar(paragraph[2])\n",
    "\n",
    "            seq_info = Sequence_Info(bana_token, bana_tag, viet_token, viet_tag, miscount)\n",
    "            sentence_lst.append(seq_info)\n",
    "\n",
    "    # Sorting the sequence info following the number of missing words\n",
    "    sentence_lst.sort(key=sort_by_missword)\n",
    "\n",
    "    # Write in the csv file\n",
    "    fstream = open(path + \"\\\\\" + contain_file + \"\\\\\" + file_name.split('.')[0] + \".csv\", 'w+', encoding='utf-8')\n",
    "    for i in sentence_lst:\n",
    "        i.write_excel(fstream)\n",
    "        fstream.write(\"\\n\")\n",
    "\n",
    "    fstream.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
