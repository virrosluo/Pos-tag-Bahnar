{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \".\\\\Bahnar_dataset\"\n",
    "folder = \"Gia Lai\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language0</th>\n",
       "      <th>language1</th>\n",
       "      <th>word_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ái ân</td>\n",
       "      <td>Tơ hưch dih băl</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ái tình</td>\n",
       "      <td>Hưch băl</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lô</td>\n",
       "      <td>Alô</td>\n",
       "      <td>đg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ăm</td>\n",
       "      <td>Amĭn</td>\n",
       "      <td>đg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ăm em</td>\n",
       "      <td>Amĭn oh; pôk</td>\n",
       "      <td>đg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language0        language1 word_type\n",
       "0     Ái ân  Tơ hưch dih băl         d\n",
       "1   Ái tình         Hưch băl         d\n",
       "2      A lô              Alô        đg\n",
       "3        Ăm             Amĭn        đg\n",
       "4     Ăm em     Amĭn oh; pôk        đg"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_path = path + \"\\\\Gia Lai Dictionary.xlsx\"\n",
    "dict_frame =pd.read_excel(dict_path)\n",
    "\n",
    "dict_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45090\n"
     ]
    }
   ],
   "source": [
    "print(dict_frame.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_word(dict, word, value):\n",
    "    if word in dict:\n",
    "        # Check for tag\n",
    "        not_Contain = 1\n",
    "        for tag in dict[word][1]:\n",
    "            if tag == value[1]:\n",
    "                not_Contain = 0\n",
    "                break\n",
    "        if not_Contain == 1:\n",
    "            dict[word][1].add(value[1])\n",
    "            dict[word][0].add(value[0])\n",
    "            # print(f\"{word} with Tag: {len(dict[word][1])}\")            \n",
    "    else:\n",
    "        dict[word] = [set(), set()]\n",
    "        dict[word][0].add(value[0])\n",
    "        dict[word][1].add(value[1])\n",
    "\n",
    "tag_dict = {\n",
    "    'A':'t',\n",
    "    'C':'k',\n",
    "    'E':'E',\n",
    "    'I':'I',\n",
    "    'L':'L',\n",
    "    'M':'M',\n",
    "    'N':'d',\n",
    "    'Nc':'Nc',\n",
    "    'Ny':'Ny',\n",
    "    'Np':'Np',\n",
    "    'Nu':'Nu',\n",
    "    'P':'đ',\n",
    "    'R':'p',\n",
    "    'S':'k',\n",
    "    'T':'tr',\n",
    "    'V':'đg',\n",
    "    'X':'x'\n",
    "}\n",
    "# Convert pyvi library Tag to Bahnar Tag\n",
    "def converter(char):\n",
    "\n",
    "    if char in tag_dict:\n",
    "        return tag_dict[char]\n",
    "    else:\n",
    "        return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ái ân'}, {'d'}]\n",
      "6600\n"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "for row in dict_frame.iterrows():\n",
    "\n",
    "    bana_word = row[1]['language1'].lower()\n",
    "    bana_word = bana_word.strip()\n",
    "\n",
    "    vietnam_word = row[1]['language0']\n",
    "\n",
    "    if(type(vietnam_word) != type(str())):\n",
    "        continue\n",
    "    else:\n",
    "        vietnam_word = vietnam_word.lower().strip()\n",
    "    vietnam_word = vietnam_word.strip()        \n",
    "\n",
    "\n",
    "    # In case we just have VietNam token but do not have bahnar token => Use VietNam token to find Bahnar token\n",
    "    if(row[1].isnull().any()):\n",
    "        token, tag = ViPosTagger.postagging(ViTokenizer.tokenize(vietnam_word))\n",
    "        if len(token) == 1:\n",
    "            # Các từ Bahnar có cùng một tiếng Việt\n",
    "            for item in bana_word.split(';'):\n",
    "                push_word(dict, item.strip(), [vietnam_word, converter(tag[0])])\n",
    "\n",
    "    # In case we have both of them, then we just push them to the dictionary\n",
    "    else:\n",
    "        tag = row[1]['word_type'].lower()\n",
    "        # Các từ Bahnar có cùng một tiếng Việt\n",
    "        for item in bana_word.split(';'):\n",
    "            push_word(dict, item.strip(), [vietnam_word, tag])\n",
    "    # break\n",
    "\n",
    "print(dict['tơ hưch dih băl'])\n",
    "print(len(dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "file_name = []\n",
    "dataset = []\n",
    "dataset_path = path + \"\\\\\" + folder\n",
    "for obj in os.scandir(dataset_path):\n",
    "    dataframe = pd.read_excel(dataset_path + \"\\\\\" + obj.name)\n",
    "    dataset.append((obj.name, dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bok Set phat rung 246-483.xlsx\n",
      "                                          Tiếng BANA  \\\n",
      "0                                                240   \n",
      "1     Hơ tôch nỡr Giông pơma, pơk bâr bok PơlăJă ...   \n",
      "2                           Manat j at kơ ih õng Inh   \n",
      "3  Bok Pơlă Jă Jal kuăr kơ dih Giông, păng nhâm p...   \n",
      "4  Sỡ ki nhi nhăp pỗ Set pơm bôl juăt đe hơnăn pỗ...   \n",
      "\n",
      "                                          TIẾNG VIỆT  \n",
      "0                                                246  \n",
      "1      Giông vừa nói xong tới ông Pơlă Jă Jal nói...  \n",
      "2                           Tội nghiệp con rể của ta  \n",
      "3  Ông Pơlă Jă Jal ôm lấy Giông bật khóc nức nờ v...  \n",
      "4  Ngày xưa ta với bạn Set kết nghĩa anh em thân ...  \n"
     ]
    }
   ],
   "source": [
    "a, b = dataset[0]\n",
    "print(a)\n",
    "print(b.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence_Info():\n",
    "    def __init__(self, bana_token, bana_tag, vietnam_token, vietnam_tag, miscount) -> None:\n",
    "        self.bana_token = bana_token\n",
    "        self.bana_tag = bana_tag\n",
    "\n",
    "        self.vietnam_token = vietnam_token\n",
    "        self.vietnam_tag = vietnam_tag\n",
    "\n",
    "        self.miscount = miscount\n",
    "\n",
    "    def get_bana(self):\n",
    "        return self.bana_token\n",
    "    \n",
    "    def get_vietnamese(self):\n",
    "        return self.vietnam_token\n",
    "    \n",
    "    def get_miscount(self):\n",
    "        return self.miscount\n",
    "    \n",
    "    def write_excel(self, fp):\n",
    "        fp.write(f\"{self.miscount}\\n\")\n",
    "\n",
    "        token_str = self.bana_token[0]\n",
    "        tag_str = '/'.join(self.bana_tag[0])\n",
    "        for token, tag in zip(self.bana_token[1:], self.bana_tag[1:]):\n",
    "            tag_str = tag_str + \",\" + '/'.join(tag)\n",
    "            token_str = token_str + \",\" + token\n",
    "\n",
    "        fp.write(f\"{token_str}\\n\")\n",
    "        fp.write(f\"{tag_str}\\n\")\n",
    "\n",
    "        fp.write(f\"{','.join(self.vietnam_token)}\\n\")\n",
    "        fp.write(f\"{','.join(self.vietnam_tag)}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_tag_vietnam(sequence):\n",
    "    token, tag = ViPosTagger.postagging(ViTokenizer.tokenize(sequence))\n",
    "    return token, tag\n",
    "\n",
    "def token_tag_bahnar(sequence):\n",
    "    token_list = []\n",
    "    tag_list = []\n",
    "\n",
    "    words = word_tokenize(sequence)\n",
    "    miscount = 0\n",
    "        \n",
    "    i = 0\n",
    "    while(i < len(words)):\n",
    "        j = 1\n",
    "        while (i + j <= len(words)):\n",
    "            if ' '.join(words[i : i + j]) in dict:\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        # In case this word is in dict => write into a file and then mark the pos tag for it\n",
    "        if j > 1:\n",
    "            token = ' '.join(words[i : i + j - 1])\n",
    "\n",
    "            token_list.append(token)\n",
    "            tag_list.append(dict[token][1])\n",
    "\n",
    "            i += (j-1)\n",
    "            \n",
    "        # In case the token is not in dict => write the token and mark it\n",
    "        else:\n",
    "            miscount += 1\n",
    "            token_list.append(words[i])\n",
    "            tag_list.append(\" \")\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    return token_list, tag_list, miscount\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_missword(info):\n",
    "    return info.miscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_file = f\"{folder}_info\"\n",
    "if not os.path.isdir(f\"{path}\\\\{contain_file}\"):\n",
    "    os.makedirs(f\"{path}\\\\{contain_file}\")\n",
    "\n",
    "# LOOPING ALL FILES IN 1 FOLDER\n",
    "for file_name, contend in dataset:\n",
    "    sentence_lst = []\n",
    "\n",
    "    # LOOPING ALL SENTENCE IN 1 FILE EXCEL\n",
    "    for idx, paragraph in contend.iterrows():\n",
    "\n",
    "        bana_sentence = paragraph['Tiếng BANA']\n",
    "        vietnam_sentence = paragraph['TIẾNG VIỆT']\n",
    "\n",
    "        # Checking for all cell do not get value\n",
    "        if type(bana_sentence) != type(str()) or type(vietnam_sentence) != type(str()):\n",
    "            continue\n",
    "\n",
    "        viet_token, viet_tag = token_tag_vietnam(vietnam_sentence.strip())\n",
    "\n",
    "        bana_token, bana_tag, miscount = token_tag_bahnar(bana_sentence.strip())\n",
    "\n",
    "        # Checking for all sentence cannot get token\n",
    "        if len(viet_token) <= 0 or len(bana_token) <= 0:\n",
    "            continue\n",
    "\n",
    "        seq_info = Sequence_Info(bana_token, bana_tag, viet_token, viet_tag, miscount)\n",
    "        sentence_lst.append(seq_info)\n",
    "\n",
    "    # Sorting the sequence info following the number of missing words\n",
    "    sentence_lst.sort(key=sort_by_missword)\n",
    "\n",
    "    # Write in the csv file\n",
    "    fstream = open(path + \"\\\\\" + contain_file + \"\\\\\" + file_name.split('.')[0] + \".csv\", 'w+', encoding='utf-8')\n",
    "    for i in sentence_lst:\n",
    "        i.write_excel(fstream)\n",
    "        fstream.write(\"\\n\")\n",
    "\n",
    "    fstream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Được', ',', 'không', 'sao', 'đâu', 'cháu'], ['V', 'F', 'R', 'P', 'R', 'N'])\n"
     ]
    }
   ],
   "source": [
    "print(token_tag_vietnam(\" Được, không sao đâu cháu\".strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
